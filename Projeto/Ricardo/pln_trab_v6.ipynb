{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ricardo/anaconda3/envs/pln_env/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import unidecode\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_val_predict, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_lg') #pt_core_news_sm\n",
    "\n",
    "SEED = 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_ID</th>\n",
       "      <th>sentence_A</th>\n",
       "      <th>sentence_B</th>\n",
       "      <th>entailment_label</th>\n",
       "      <th>relatedness_score</th>\n",
       "      <th>entailment_AB</th>\n",
       "      <th>entailment_BA</th>\n",
       "      <th>sentence_A_original</th>\n",
       "      <th>sentence_B_original</th>\n",
       "      <th>sentence_A_dataset</th>\n",
       "      <th>sentence_B_dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Um grupo de crianças está brincando em um quin...</td>\n",
       "      <td>Um grupo de meninos em um quintal está brincan...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>4.5</td>\n",
       "      <td>A_neutral_B</td>\n",
       "      <td>B_neutral_A</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>A group of boys in a yard is playing and a man...</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>FLICKR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Um grupo de crianças está brincando na casa e ...</td>\n",
       "      <td>Um grupo de crianças está brincando em um quin...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>3.2</td>\n",
       "      <td>A_contradicts_B</td>\n",
       "      <td>B_neutral_A</td>\n",
       "      <td>A group of children is playing in the house an...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>FLICKR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Os meninos jovens estão brincando ao ar livre ...</td>\n",
       "      <td>As crianças estão brincando ao ar livre perto ...</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>4.7</td>\n",
       "      <td>A_entails_B</td>\n",
       "      <td>B_entails_A</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>The kids are playing outdoors near a man with ...</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>FLICKR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Os meninos jovens estão brincando ao ar livre ...</td>\n",
       "      <td>Não tem nenhum menino brincando ao ar livre e ...</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "      <td>3.6</td>\n",
       "      <td>A_contradicts_B</td>\n",
       "      <td>B_contradicts_A</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>There is no boy playing outdoors and there is ...</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>FLICKR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>As crianças estão brincando ao ar livre perto ...</td>\n",
       "      <td>Um grupo de crianças está brincando em um quin...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>3.4</td>\n",
       "      <td>A_neutral_B</td>\n",
       "      <td>B_neutral_A</td>\n",
       "      <td>The kids are playing outdoors near a man with ...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>FLICKR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pair_ID                                         sentence_A  \\\n",
       "0        1  Um grupo de crianças está brincando em um quin...   \n",
       "1        2  Um grupo de crianças está brincando na casa e ...   \n",
       "2        3  Os meninos jovens estão brincando ao ar livre ...   \n",
       "3        4  Os meninos jovens estão brincando ao ar livre ...   \n",
       "4        5  As crianças estão brincando ao ar livre perto ...   \n",
       "\n",
       "                                          sentence_B entailment_label  \\\n",
       "0  Um grupo de meninos em um quintal está brincan...          NEUTRAL   \n",
       "1  Um grupo de crianças está brincando em um quin...          NEUTRAL   \n",
       "2  As crianças estão brincando ao ar livre perto ...       ENTAILMENT   \n",
       "3  Não tem nenhum menino brincando ao ar livre e ...    CONTRADICTION   \n",
       "4  Um grupo de crianças está brincando em um quin...          NEUTRAL   \n",
       "\n",
       "   relatedness_score    entailment_AB    entailment_BA  \\\n",
       "0                4.5      A_neutral_B      B_neutral_A   \n",
       "1                3.2  A_contradicts_B      B_neutral_A   \n",
       "2                4.7      A_entails_B      B_entails_A   \n",
       "3                3.6  A_contradicts_B  B_contradicts_A   \n",
       "4                3.4      A_neutral_B      B_neutral_A   \n",
       "\n",
       "                                 sentence_A_original  \\\n",
       "0  A group of kids is playing in a yard and an ol...   \n",
       "1  A group of children is playing in the house an...   \n",
       "2  The young boys are playing outdoors and the ma...   \n",
       "3  The young boys are playing outdoors and the ma...   \n",
       "4  The kids are playing outdoors near a man with ...   \n",
       "\n",
       "                                 sentence_B_original sentence_A_dataset  \\\n",
       "0  A group of boys in a yard is playing and a man...             FLICKR   \n",
       "1  A group of kids is playing in a yard and an ol...             FLICKR   \n",
       "2  The kids are playing outdoors near a man with ...             FLICKR   \n",
       "3  There is no boy playing outdoors and there is ...             FLICKR   \n",
       "4  A group of kids is playing in a yard and an ol...             FLICKR   \n",
       "\n",
       "  sentence_B_dataset  \n",
       "0             FLICKR  \n",
       "1             FLICKR  \n",
       "2             FLICKR  \n",
       "3             FLICKR  \n",
       "4             FLICKR  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dados/SICK_BR_Train.csv', delimiter=';', encoding='mac_roman')\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8840 entries, 0 to 8839\n",
      "Data columns (total 11 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   pair_ID              8840 non-null   int64  \n",
      " 1   sentence_A           8840 non-null   object \n",
      " 2   sentence_B           8840 non-null   object \n",
      " 3   entailment_label     8840 non-null   object \n",
      " 4   relatedness_score    8840 non-null   float64\n",
      " 5   entailment_AB        8840 non-null   object \n",
      " 6   entailment_BA        8840 non-null   object \n",
      " 7   sentence_A_original  8840 non-null   object \n",
      " 8   sentence_B_original  8840 non-null   object \n",
      " 9   sentence_A_dataset   8840 non-null   object \n",
      " 10  sentence_B_dataset   8840 non-null   object \n",
      "dtypes: float64(1), int64(1), object(9)\n",
      "memory usage: 759.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_ID</th>\n",
       "      <th>sentence_A</th>\n",
       "      <th>sentence_B</th>\n",
       "      <th>entailment_label</th>\n",
       "      <th>relatedness_score</th>\n",
       "      <th>entailment_AB</th>\n",
       "      <th>entailment_BA</th>\n",
       "      <th>sentence_A_original</th>\n",
       "      <th>sentence_B_original</th>\n",
       "      <th>sentence_A_dataset</th>\n",
       "      <th>sentence_B_dataset</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Um grupo de crianças está brincando em um quin...</td>\n",
       "      <td>Um grupo de meninos em um quintal está brincan...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>4.5</td>\n",
       "      <td>A_neutral_B</td>\n",
       "      <td>B_neutral_A</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>A group of boys in a yard is playing and a man...</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Um grupo de crianças está brincando na casa e ...</td>\n",
       "      <td>Um grupo de crianças está brincando em um quin...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>3.2</td>\n",
       "      <td>A_contradicts_B</td>\n",
       "      <td>B_neutral_A</td>\n",
       "      <td>A group of children is playing in the house an...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Os meninos jovens estão brincando ao ar livre ...</td>\n",
       "      <td>As crianças estão brincando ao ar livre perto ...</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>4.7</td>\n",
       "      <td>A_entails_B</td>\n",
       "      <td>B_entails_A</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>The kids are playing outdoors near a man with ...</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Os meninos jovens estão brincando ao ar livre ...</td>\n",
       "      <td>Não tem nenhum menino brincando ao ar livre e ...</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "      <td>3.6</td>\n",
       "      <td>A_contradicts_B</td>\n",
       "      <td>B_contradicts_A</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>There is no boy playing outdoors and there is ...</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>As crianças estão brincando ao ar livre perto ...</td>\n",
       "      <td>Um grupo de crianças está brincando em um quin...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>3.4</td>\n",
       "      <td>A_neutral_B</td>\n",
       "      <td>B_neutral_A</td>\n",
       "      <td>The kids are playing outdoors near a man with ...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>FLICKR</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pair_ID                                         sentence_A  \\\n",
       "0        1  Um grupo de crianças está brincando em um quin...   \n",
       "1        2  Um grupo de crianças está brincando na casa e ...   \n",
       "2        3  Os meninos jovens estão brincando ao ar livre ...   \n",
       "3        4  Os meninos jovens estão brincando ao ar livre ...   \n",
       "4        5  As crianças estão brincando ao ar livre perto ...   \n",
       "\n",
       "                                          sentence_B entailment_label  \\\n",
       "0  Um grupo de meninos em um quintal está brincan...          NEUTRAL   \n",
       "1  Um grupo de crianças está brincando em um quin...          NEUTRAL   \n",
       "2  As crianças estão brincando ao ar livre perto ...       ENTAILMENT   \n",
       "3  Não tem nenhum menino brincando ao ar livre e ...    CONTRADICTION   \n",
       "4  Um grupo de crianças está brincando em um quin...          NEUTRAL   \n",
       "\n",
       "   relatedness_score    entailment_AB    entailment_BA  \\\n",
       "0                4.5      A_neutral_B      B_neutral_A   \n",
       "1                3.2  A_contradicts_B      B_neutral_A   \n",
       "2                4.7      A_entails_B      B_entails_A   \n",
       "3                3.6  A_contradicts_B  B_contradicts_A   \n",
       "4                3.4      A_neutral_B      B_neutral_A   \n",
       "\n",
       "                                 sentence_A_original  \\\n",
       "0  A group of kids is playing in a yard and an ol...   \n",
       "1  A group of children is playing in the house an...   \n",
       "2  The young boys are playing outdoors and the ma...   \n",
       "3  The young boys are playing outdoors and the ma...   \n",
       "4  The kids are playing outdoors near a man with ...   \n",
       "\n",
       "                                 sentence_B_original sentence_A_dataset  \\\n",
       "0  A group of boys in a yard is playing and a man...             FLICKR   \n",
       "1  A group of kids is playing in a yard and an ol...             FLICKR   \n",
       "2  The kids are playing outdoors near a man with ...             FLICKR   \n",
       "3  There is no boy playing outdoors and there is ...             FLICKR   \n",
       "4  A group of kids is playing in a yard and an ol...             FLICKR   \n",
       "\n",
       "  sentence_B_dataset  label  \n",
       "0             FLICKR      0  \n",
       "1             FLICKR      0  \n",
       "2             FLICKR      1  \n",
       "3             FLICKR      2  \n",
       "4             FLICKR      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_label_mapper = {'NEUTRAL':0, \n",
    "                      'ENTAILMENT':1,\n",
    "                      'CONTRADICTION':2}\n",
    "\n",
    "df['label'] = df['entailment_label'].replace(scale_label_mapper)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanceamento das classes (downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5261\n",
       "1    2488\n",
       "2    1091\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando o balanceamento das classes\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1091\n",
       "2    1091\n",
       "1    1091\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "df_maior = df[df['label']==0]\n",
    "qtd_maior = len(df_maior)\n",
    "\n",
    "df_meio = df[df['label']==1]\n",
    "qtd_meio = len(df_meio)\n",
    "\n",
    "df_menor = df[df['label']==2]\n",
    "qtd_menor = len(df_menor)\n",
    "\n",
    "df_maior_down = resample(df_maior, replace=False, n_samples=qtd_menor, random_state=SEED)\n",
    "df_meio_down = resample(df_meio, replace=False, n_samples=qtd_menor, random_state=SEED)\n",
    "\n",
    "df = pd.concat([df_menor, df_meio_down, df_maior_down])\n",
    "\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_ID</th>\n",
       "      <th>sentence_A</th>\n",
       "      <th>sentence_B</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Os meninos jovens estão brincando ao ar livre ...</td>\n",
       "      <td>Não tem nenhum menino brincando ao ar livre e ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Dois cachorros estão lutando e se abraçando</td>\n",
       "      <td>Não tem nenhum cachorro lutando e abraçando</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Não há nenhum homem de jaqueta preta fazendo t...</td>\n",
       "      <td>Uma pessoa de blusa preta está fazendo truques...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Ninguém está dirigindo uma bicicleta com uma roda</td>\n",
       "      <td>Uma pessoa está andando de bicicleta em uma só...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Um homem com uma camisa de time está enterrand...</td>\n",
       "      <td>Não tem nenhum homem jogando a bola em um jogo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pair_ID                                         sentence_A  \\\n",
       "3         4  Os meninos jovens estão brincando ao ar livre ...   \n",
       "12       13        Dois cachorros estão lutando e se abraçando   \n",
       "19       20  Não há nenhum homem de jaqueta preta fazendo t...   \n",
       "22       23  Ninguém está dirigindo uma bicicleta com uma roda   \n",
       "30       31  Um homem com uma camisa de time está enterrand...   \n",
       "\n",
       "                                           sentence_B  label  \n",
       "3   Não tem nenhum menino brincando ao ar livre e ...      2  \n",
       "12        Não tem nenhum cachorro lutando e abraçando      2  \n",
       "19  Uma pessoa de blusa preta está fazendo truques...      2  \n",
       "22  Uma pessoa está andando de bicicleta em uma só...      2  \n",
       "30  Não tem nenhum homem jogando a bola em um jogo...      2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['relatedness_score', 'entailment_label',\n",
    "                      'entailment_AB', 'entailment_BA',\n",
    "                      'sentence_A_original', 'sentence_B_original', \n",
    "                      'sentence_A_dataset', 'sentence_B_dataset'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento, Steemer, Lematização e POS Tagger, NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_texto(sentenca):\n",
    "    sentenca = re.sub(r'\\n', '', sentenca) #remove as quebras de linha\n",
    "    sentenca = re.sub(r'\\t', ' ', sentenca) #substitui tabulações por um espaço em branco\n",
    "    sentenca = re.sub(r'\\s+', ' ', sentenca, flags=re.I) #substitui um ou mais espaços em branco por um espaço\n",
    "    sentenca = re.sub('[\"‘’“”…]', '', sentenca) #remove aspas e apóstofres\n",
    "    return unidecode.unidecode(sentenca.lower()) #remove acentos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizar_texto(sentenca):\n",
    "    tokens = nlp(sentenca)\n",
    "    ners = [(entity.text, entity.label_) for entity in tokens.ents]\n",
    "    chunkers = [(token.text, token.dep_, token.head.text, token.head.pos_, [child for child in token.children]) for token in tokens]\n",
    "    tokens = [token for token in tokens if not token.is_punct]\n",
    "    tokens = [token for token in tokens if not token.is_stop] \n",
    "    postags = [token.pos_ for token in tokens]\n",
    "    lemmas = [token.lemma_.lower().strip() for token in tokens]\n",
    "    \n",
    "    ners = ' '.join([str(item) for item in ners]) \n",
    "    postags = ' '.join([str(item) for item in postags]) \n",
    "    lemmas = ' '.join([str(item) for item in lemmas]) \n",
    "    chunkers = ' '.join([str(item) for item in chunkers]) \n",
    "    \n",
    "    return ners, postags, lemmas, chunkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_grams(words, n):\n",
    "    grams = []\n",
    "    for ngram in ngrams(words, n):\n",
    "        grams.append(' '.join(str(i) for i in ngram))\n",
    "    grams = ' '.join([str(item) for item in grams]) \n",
    "    return grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "menino jovem estao brincar ar livrar homem sorrir\n"
     ]
    }
   ],
   "source": [
    "texto_exemplo = {}\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    a_normalizado = normalizar_texto(df.iloc[i, 1])\n",
    "    b_normalizado = normalizar_texto(df.iloc[i, 2])\n",
    "                                     \n",
    "    a_ners, a_postags, a_lemmas, a_chunkers = tokenizar_texto(a_normalizado)\n",
    "    b_ners, b_postags, b_lemmas, b_chunkers = tokenizar_texto(b_normalizado)\n",
    "                                     \n",
    "    sentence_a_unigrama = word_grams(a_normalizado.split(), 1)\n",
    "    sentence_b_unigrama = word_grams(b_normalizado.split(), 1)\n",
    "    \n",
    "    sentence_a_bigrama = word_grams(a_normalizado.split(), 2)\n",
    "    sentence_b_bigrama = word_grams(b_normalizado.split(), 2)\n",
    "    \n",
    "    texto_exemplo[i] = {'id': df.iloc[i, 0],\n",
    "                        'sentenca_a': df.iloc[i, 1],\n",
    "                        'sentenca_b': df.iloc[i, 2],\n",
    "                        'label': df.iloc[i, 3],\n",
    "                        'sentenca_a_normalizado': a_normalizado,\n",
    "                        'sentenca_b_normalizado': b_normalizado,\n",
    "                        'sentenca_a_ner': a_ners,\n",
    "                        'sentenca_b_ner': b_ners,\n",
    "                        'sentenca_a_postag': a_postags,\n",
    "                        'sentenca_b_postag': b_postags,\n",
    "                        'sentenca_a_lemma': a_lemmas,\n",
    "                        'sentenca_b_lemma': b_lemmas,\n",
    "                        'sentenca_a_chunker': a_chunkers,\n",
    "                        'sentenca_b_chunker': b_chunkers,\n",
    "                        'sentenca_a_unigrama': sentence_a_unigrama,\n",
    "                        'sentenca_b_unigrama': sentence_b_unigrama,\n",
    "                        'sentenca_a_bigrama': sentence_a_bigrama,\n",
    "                        'sentenca_b_bigrama': sentence_b_bigrama}\n",
    "\n",
    "print(texto_exemplo[0]['sentenca_a_lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vetorização das features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'dados/skip_s300.txt'\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=False, unicode_errors=\"ignore\")\n",
    "\n",
    "def word2vec_embedding(sentenca):\n",
    "    word2vec_words = []\n",
    "    for word in sentenca.split():\n",
    "        try:\n",
    "            word2vec_word = word2vec[word]\n",
    "            word2vec_words.append(word2vec_word)\n",
    "        except KeyError:\n",
    "            'word ' + word + ' not in vocabulary'\n",
    "    sentence_word2vec = list(map(sum, zip(*word2vec_words)))\n",
    "    return sentence_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentenca_a</th>\n",
       "      <th>sentenca_b</th>\n",
       "      <th>label</th>\n",
       "      <th>sentenca_a_normalizado</th>\n",
       "      <th>sentenca_b_normalizado</th>\n",
       "      <th>sentenca_a_ner</th>\n",
       "      <th>sentenca_b_ner</th>\n",
       "      <th>sentenca_a_postag</th>\n",
       "      <th>sentenca_b_postag</th>\n",
       "      <th>sentenca_a_lemma</th>\n",
       "      <th>sentenca_b_lemma</th>\n",
       "      <th>sentenca_a_chunker</th>\n",
       "      <th>sentenca_b_chunker</th>\n",
       "      <th>sentenca_a_unigrama</th>\n",
       "      <th>sentenca_b_unigrama</th>\n",
       "      <th>sentenca_a_bigrama</th>\n",
       "      <th>sentenca_b_bigrama</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3268</th>\n",
       "      <td>9241</td>\n",
       "      <td>O homem de azul está posando para uma foto ao ...</td>\n",
       "      <td>A mulher de azul está posando para uma foto pe...</td>\n",
       "      <td>0</td>\n",
       "      <td>o homem de azul esta posando para uma foto ao ...</td>\n",
       "      <td>a mulher de azul esta posando para uma foto pe...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NOUN ADJ VERB NOUN NOUN</td>\n",
       "      <td>NOUN NOUN VERB NOUN NOUN</td>\n",
       "      <td>homem azul posar foto carro</td>\n",
       "      <td>mulher azul posar foto carro</td>\n",
       "      <td>('o', 'det', 'homem', 'NOUN', []) ('homem', 'n...</td>\n",
       "      <td>('a', 'det', 'mulher', 'NOUN', []) ('mulher', ...</td>\n",
       "      <td>o homem de azul esta posando para uma foto ao ...</td>\n",
       "      <td>a mulher de azul esta posando para uma foto pe...</td>\n",
       "      <td>o homem homem de de azul azul esta esta posand...</td>\n",
       "      <td>a mulher mulher de de azul azul esta esta posa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3269</th>\n",
       "      <td>3459</td>\n",
       "      <td>O homem está cortando um tronco de árvore com ...</td>\n",
       "      <td>Um homem está picando um tronco com um machado</td>\n",
       "      <td>0</td>\n",
       "      <td>o homem esta cortando um tronco de arvore com ...</td>\n",
       "      <td>um homem esta picando um tronco com um machado</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NOUN VERB NOUN NOUN NOUN</td>\n",
       "      <td>NOUN VERB NOUN NOUN</td>\n",
       "      <td>homem cortar troncar arvorar machadar</td>\n",
       "      <td>homem picar troncar machadar</td>\n",
       "      <td>('o', 'det', 'homem', 'NOUN', []) ('homem', 'R...</td>\n",
       "      <td>('um', 'det', 'homem', 'NOUN', []) ('homem', '...</td>\n",
       "      <td>o homem esta cortando um tronco de arvore com ...</td>\n",
       "      <td>um homem esta picando um tronco com um machado</td>\n",
       "      <td>o homem homem esta esta cortando cortando um u...</td>\n",
       "      <td>um homem homem esta esta picando picando um um...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270</th>\n",
       "      <td>6730</td>\n",
       "      <td>Uma criança está esmagando uma bola de neve</td>\n",
       "      <td>Uma criança de laranja está brincando com uma ...</td>\n",
       "      <td>0</td>\n",
       "      <td>uma crianca esta esmagando uma bola de neve</td>\n",
       "      <td>uma crianca de laranja esta brincando com uma ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NOUN VERB NOUN NOUN</td>\n",
       "      <td>NOUN NOUN VERB NOUN NOUN</td>\n",
       "      <td>crianca esmagar bolar nevar</td>\n",
       "      <td>crianca laranjo brincar bolar nevar</td>\n",
       "      <td>('uma', 'det', 'crianca', 'NOUN', []) ('crianc...</td>\n",
       "      <td>('uma', 'det', 'crianca', 'NOUN', []) ('crianc...</td>\n",
       "      <td>uma crianca esta esmagando uma bola de neve</td>\n",
       "      <td>uma crianca de laranja esta brincando com uma ...</td>\n",
       "      <td>uma crianca crianca esta esta esmagando esmaga...</td>\n",
       "      <td>uma crianca crianca de de laranja laranja esta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3271</th>\n",
       "      <td>6697</td>\n",
       "      <td>Um grupo de pessoas está sentado em ambos os l...</td>\n",
       "      <td>Um grupo de pessoas está parado em ambos os la...</td>\n",
       "      <td>0</td>\n",
       "      <td>um grupo de pessoas esta sentado em ambos os l...</td>\n",
       "      <td>um grupo de pessoas esta parado em ambos os la...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NOUN VERB NOUN NOUN NOUN ADJ</td>\n",
       "      <td>NOUN VERB NOUN NOUN NOUN ADJ</td>\n",
       "      <td>pessoa sentar lado estruturar pedrar vermelho</td>\n",
       "      <td>pessoa parar lado estruturar pedrar vermelho</td>\n",
       "      <td>('um', 'det', 'grupo', 'NOUN', []) ('grupo', '...</td>\n",
       "      <td>('um', 'det', 'grupo', 'NOUN', []) ('grupo', '...</td>\n",
       "      <td>um grupo de pessoas esta sentado em ambos os l...</td>\n",
       "      <td>um grupo de pessoas esta parado em ambos os la...</td>\n",
       "      <td>um grupo grupo de de pessoas pessoas esta esta...</td>\n",
       "      <td>um grupo grupo de de pessoas pessoas esta esta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3272</th>\n",
       "      <td>6549</td>\n",
       "      <td>Um menino pequenino está parado na floresta</td>\n",
       "      <td>Um menino pequenino está sentado na floresta</td>\n",
       "      <td>0</td>\n",
       "      <td>um menino pequenino esta parado na floresta</td>\n",
       "      <td>um menino pequenino esta sentado na floresta</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NOUN ADJ VERB NOUN</td>\n",
       "      <td>NOUN ADJ VERB NOUN</td>\n",
       "      <td>menino pequenino parar florestar</td>\n",
       "      <td>menino pequenino sentar florestar</td>\n",
       "      <td>('um', 'det', 'menino', 'NOUN', []) ('menino',...</td>\n",
       "      <td>('um', 'det', 'menino', 'NOUN', []) ('menino',...</td>\n",
       "      <td>um menino pequenino esta parado na floresta</td>\n",
       "      <td>um menino pequenino esta sentado na floresta</td>\n",
       "      <td>um menino menino pequenino pequenino esta esta...</td>\n",
       "      <td>um menino menino pequenino pequenino esta esta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                         sentenca_a  \\\n",
       "3268  9241  O homem de azul está posando para uma foto ao ...   \n",
       "3269  3459  O homem está cortando um tronco de árvore com ...   \n",
       "3270  6730        Uma criança está esmagando uma bola de neve   \n",
       "3271  6697  Um grupo de pessoas está sentado em ambos os l...   \n",
       "3272  6549        Um menino pequenino está parado na floresta   \n",
       "\n",
       "                                             sentenca_b label  \\\n",
       "3268  A mulher de azul está posando para uma foto pe...     0   \n",
       "3269     Um homem está picando um tronco com um machado     0   \n",
       "3270  Uma criança de laranja está brincando com uma ...     0   \n",
       "3271  Um grupo de pessoas está parado em ambos os la...     0   \n",
       "3272       Um menino pequenino está sentado na floresta     0   \n",
       "\n",
       "                                 sentenca_a_normalizado  \\\n",
       "3268  o homem de azul esta posando para uma foto ao ...   \n",
       "3269  o homem esta cortando um tronco de arvore com ...   \n",
       "3270        uma crianca esta esmagando uma bola de neve   \n",
       "3271  um grupo de pessoas esta sentado em ambos os l...   \n",
       "3272        um menino pequenino esta parado na floresta   \n",
       "\n",
       "                                 sentenca_b_normalizado sentenca_a_ner  \\\n",
       "3268  a mulher de azul esta posando para uma foto pe...                  \n",
       "3269     um homem esta picando um tronco com um machado                  \n",
       "3270  uma crianca de laranja esta brincando com uma ...                  \n",
       "3271  um grupo de pessoas esta parado em ambos os la...                  \n",
       "3272       um menino pequenino esta sentado na floresta                  \n",
       "\n",
       "     sentenca_b_ner             sentenca_a_postag  \\\n",
       "3268                      NOUN ADJ VERB NOUN NOUN   \n",
       "3269                     NOUN VERB NOUN NOUN NOUN   \n",
       "3270                          NOUN VERB NOUN NOUN   \n",
       "3271                 NOUN VERB NOUN NOUN NOUN ADJ   \n",
       "3272                           NOUN ADJ VERB NOUN   \n",
       "\n",
       "                 sentenca_b_postag  \\\n",
       "3268      NOUN NOUN VERB NOUN NOUN   \n",
       "3269           NOUN VERB NOUN NOUN   \n",
       "3270      NOUN NOUN VERB NOUN NOUN   \n",
       "3271  NOUN VERB NOUN NOUN NOUN ADJ   \n",
       "3272            NOUN ADJ VERB NOUN   \n",
       "\n",
       "                                   sentenca_a_lemma  \\\n",
       "3268                    homem azul posar foto carro   \n",
       "3269          homem cortar troncar arvorar machadar   \n",
       "3270                    crianca esmagar bolar nevar   \n",
       "3271  pessoa sentar lado estruturar pedrar vermelho   \n",
       "3272               menino pequenino parar florestar   \n",
       "\n",
       "                                  sentenca_b_lemma  \\\n",
       "3268                  mulher azul posar foto carro   \n",
       "3269                  homem picar troncar machadar   \n",
       "3270           crianca laranjo brincar bolar nevar   \n",
       "3271  pessoa parar lado estruturar pedrar vermelho   \n",
       "3272             menino pequenino sentar florestar   \n",
       "\n",
       "                                     sentenca_a_chunker  \\\n",
       "3268  ('o', 'det', 'homem', 'NOUN', []) ('homem', 'n...   \n",
       "3269  ('o', 'det', 'homem', 'NOUN', []) ('homem', 'R...   \n",
       "3270  ('uma', 'det', 'crianca', 'NOUN', []) ('crianc...   \n",
       "3271  ('um', 'det', 'grupo', 'NOUN', []) ('grupo', '...   \n",
       "3272  ('um', 'det', 'menino', 'NOUN', []) ('menino',...   \n",
       "\n",
       "                                     sentenca_b_chunker  \\\n",
       "3268  ('a', 'det', 'mulher', 'NOUN', []) ('mulher', ...   \n",
       "3269  ('um', 'det', 'homem', 'NOUN', []) ('homem', '...   \n",
       "3270  ('uma', 'det', 'crianca', 'NOUN', []) ('crianc...   \n",
       "3271  ('um', 'det', 'grupo', 'NOUN', []) ('grupo', '...   \n",
       "3272  ('um', 'det', 'menino', 'NOUN', []) ('menino',...   \n",
       "\n",
       "                                    sentenca_a_unigrama  \\\n",
       "3268  o homem de azul esta posando para uma foto ao ...   \n",
       "3269  o homem esta cortando um tronco de arvore com ...   \n",
       "3270        uma crianca esta esmagando uma bola de neve   \n",
       "3271  um grupo de pessoas esta sentado em ambos os l...   \n",
       "3272        um menino pequenino esta parado na floresta   \n",
       "\n",
       "                                    sentenca_b_unigrama  \\\n",
       "3268  a mulher de azul esta posando para uma foto pe...   \n",
       "3269     um homem esta picando um tronco com um machado   \n",
       "3270  uma crianca de laranja esta brincando com uma ...   \n",
       "3271  um grupo de pessoas esta parado em ambos os la...   \n",
       "3272       um menino pequenino esta sentado na floresta   \n",
       "\n",
       "                                     sentenca_a_bigrama  \\\n",
       "3268  o homem homem de de azul azul esta esta posand...   \n",
       "3269  o homem homem esta esta cortando cortando um u...   \n",
       "3270  uma crianca crianca esta esta esmagando esmaga...   \n",
       "3271  um grupo grupo de de pessoas pessoas esta esta...   \n",
       "3272  um menino menino pequenino pequenino esta esta...   \n",
       "\n",
       "                                     sentenca_b_bigrama  \n",
       "3268  a mulher mulher de de azul azul esta esta posa...  \n",
       "3269  um homem homem esta esta picando picando um um...  \n",
       "3270  uma crianca crianca de de laranja laranja esta...  \n",
       "3271  um grupo grupo de de pessoas pessoas esta esta...  \n",
       "3272  um menino menino pequenino pequenino esta esta...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trab = pd.DataFrame(texto_exemplo).T\n",
    "\n",
    "df_trab.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3273, 300)\n"
     ]
    }
   ],
   "source": [
    "w2v_sentence_a = []\n",
    "w2v_sentence_b = []\n",
    "\n",
    "for i in range(df_trab.shape[0]):\n",
    "    w2v_sentence_a.append(word2vec_embedding(df_trab.iloc[i, 4]))\n",
    "    w2v_sentence_b.append(word2vec_embedding(df_trab.iloc[i, 5]))\n",
    "\n",
    "df_w2v_sentenca_a = pd.DataFrame(w2v_sentence_a, columns=['sent_a_w2v_' + str(i) for i in range(1, len(w2v_sentence_a[1]) + 1)])\n",
    "df_w2v_sentenca_b = pd.DataFrame(w2v_sentence_a, columns=['sent_b_w2v_' + str(i) for i in range(1, len(w2v_sentence_b[1]) + 1)])\n",
    "\n",
    "print(df_w2v_sentenca_a.shape)\n",
    "# df_w2v_sentenca_a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3273, 15147)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features=2000)\n",
    "\n",
    "lemma_a_vector = tfidf.fit_transform(df_trab['sentenca_a_lemma'])\n",
    "lemma_b_vector = tfidf.fit_transform(df_trab['sentenca_b_lemma'])\n",
    "postag_a_vector = tfidf.fit_transform(df_trab['sentenca_a_postag'])\n",
    "postag_b_vector = tfidf.fit_transform(df_trab['sentenca_b_postag'])\n",
    "chunker_a_vector = tfidf.fit_transform(df_trab['sentenca_a_chunker'])\n",
    "chunker_b_vector = tfidf.fit_transform(df_trab['sentenca_b_chunker'])\n",
    "unigrama_a_vector = tfidf.fit_transform(df_trab['sentenca_a_unigrama'])\n",
    "unigrama_b_vector = tfidf.fit_transform(df_trab['sentenca_b_unigrama'])\n",
    "bigrama_a_vector = tfidf.fit_transform(df_trab['sentenca_a_bigrama'])\n",
    "bigrama_b_vector = tfidf.fit_transform(df_trab['sentenca_b_bigrama'])\n",
    "\n",
    "X = np.hstack((lemma_a_vector.todense(),\n",
    "               lemma_b_vector.todense(), \n",
    "               postag_a_vector.todense(), \n",
    "               postag_b_vector.todense(),\n",
    "               chunker_a_vector.todense(), \n",
    "               chunker_b_vector.todense(), \n",
    "               unigrama_a_vector.todense(), \n",
    "               unigrama_b_vector.todense(), \n",
    "               bigrama_a_vector.todense(),\n",
    "               bigrama_b_vector.todense(),\n",
    "               df_w2v_sentenca_a, \n",
    "               df_w2v_sentenca_b))\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento do modelo e análise dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_trab['label']\n",
    "y = y.astype('int')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 15,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 7,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': 46,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "rfc_model = RandomForestClassifier(max_depth=15, min_samples_leaf=2, min_samples_split=7, random_state=SEED).fit(X_train, y_train)\n",
    "\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rfc_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (Treino):  0.9711916193801833\n",
      "Random Forest (Teste):  0.5763747454175153\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest (Treino): ',rfc_model.score(X_train, y_train))\n",
    "y_pred = rfc_model.predict(X_test)\n",
    "print('Random Forest (Teste): ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusão do Random Forest: \n",
      "                NEUTRAL  ENTAILMENT  CONTRADICTION\n",
      "NEUTRAL            111         176             48\n",
      "ENTAILMENT         106         192             13\n",
      "CONTRADICTION       36          37            263\n",
      "\n",
      "\n",
      "Avaliação do Random Forest: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.33      0.38       335\n",
      "           1       0.47      0.62      0.54       311\n",
      "           2       0.81      0.78      0.80       336\n",
      "\n",
      "    accuracy                           0.58       982\n",
      "   macro avg       0.57      0.58      0.57       982\n",
      "weighted avg       0.58      0.58      0.57       982\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = pd.DataFrame(confusion_matrix(y_test, y_pred), columns=scale_label_mapper.keys(), index=scale_label_mapper.keys())\n",
    "\n",
    "print('Matriz de Confusão do Random Forest: \\n', cm)\n",
    "print('\\n\\nAvaliação do Random Forest: \\n', classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinando e avaliando o Random Forest com validação cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média (CV): 0.576\n",
      "Intervalo: [0.552, 0.599]\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "rfc_model = RandomForestClassifier(max_depth=15, min_samples_leaf=2, min_samples_split=7, random_state=SEED)\n",
    "\n",
    "scores = cross_val_score(rfc_model, X, y, cv=cv)\n",
    "\n",
    "score_medio = scores.mean()\n",
    "score_desvio = scores.std()\n",
    "\n",
    "print('Acurácia média (CV): %.3f' % score_medio)\n",
    "print('Intervalo: [%.3f, %.3f]' % (score_medio - 2 * score_desvio, score_medio + 2 * score_desvio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusão do Random Forest (CV): \n",
      "                NEUTRAL  ENTAILMENT  CONTRADICTION\n",
      "NEUTRAL            406         499            186\n",
      "ENTAILMENT         405         639             47\n",
      "CONTRADICTION      128         124            839\n",
      "\n",
      "\n",
      "Avaliação do Random Forest (CV): \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.37      0.40      1091\n",
      "           1       0.51      0.59      0.54      1091\n",
      "           2       0.78      0.77      0.78      1091\n",
      "\n",
      "    accuracy                           0.58      3273\n",
      "   macro avg       0.57      0.58      0.57      3273\n",
      "weighted avg       0.57      0.58      0.57      3273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_preds = cross_val_predict(rfc_model, X, y, cv=cv)\n",
    "\n",
    "cm = pd.DataFrame(confusion_matrix(y, y_preds), columns=scale_label_mapper.keys(), index=scale_label_mapper.keys())\n",
    "\n",
    "print('Matriz de Confusão do Random Forest (CV): \\n', cm)\n",
    "print('\\n\\nAvaliação do Random Forest (CV): \\n', classification_report(y, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 15,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 7,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': 46,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print('Parameters currently in use:\\n')\n",
    "pprint(rfc_model.get_params())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajustando os hiperparâmetros do RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parametros :  {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 7, 'n_estimators': 150}\n",
      "Melhor score:  0.5915003384924248\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.594552</td>\n",
       "      <td>0.015685</td>\n",
       "      <td>0.029260</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 1, 'min_...</td>\n",
       "      <td>0.596947</td>\n",
       "      <td>0.586260</td>\n",
       "      <td>0.554198</td>\n",
       "      <td>0.579511</td>\n",
       "      <td>0.567278</td>\n",
       "      <td>0.576839</td>\n",
       "      <td>0.014861</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.047000</td>\n",
       "      <td>0.013573</td>\n",
       "      <td>0.038398</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 1, 'min_...</td>\n",
       "      <td>0.598473</td>\n",
       "      <td>0.592366</td>\n",
       "      <td>0.560305</td>\n",
       "      <td>0.590214</td>\n",
       "      <td>0.585627</td>\n",
       "      <td>0.585397</td>\n",
       "      <td>0.013209</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.482934</td>\n",
       "      <td>0.008807</td>\n",
       "      <td>0.047124</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 1, 'min_...</td>\n",
       "      <td>0.590840</td>\n",
       "      <td>0.616794</td>\n",
       "      <td>0.558779</td>\n",
       "      <td>0.585627</td>\n",
       "      <td>0.602446</td>\n",
       "      <td>0.590897</td>\n",
       "      <td>0.019305</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.575366</td>\n",
       "      <td>0.004644</td>\n",
       "      <td>0.029168</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 1, 'min_...</td>\n",
       "      <td>0.595420</td>\n",
       "      <td>0.607634</td>\n",
       "      <td>0.563359</td>\n",
       "      <td>0.588685</td>\n",
       "      <td>0.570336</td>\n",
       "      <td>0.585087</td>\n",
       "      <td>0.016234</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.011917</td>\n",
       "      <td>0.011227</td>\n",
       "      <td>0.038349</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 1, 'min_...</td>\n",
       "      <td>0.596947</td>\n",
       "      <td>0.609160</td>\n",
       "      <td>0.560305</td>\n",
       "      <td>0.582569</td>\n",
       "      <td>0.585627</td>\n",
       "      <td>0.586922</td>\n",
       "      <td>0.016273</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.442428</td>\n",
       "      <td>0.014459</td>\n",
       "      <td>0.046950</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 1, 'min_...</td>\n",
       "      <td>0.604580</td>\n",
       "      <td>0.619847</td>\n",
       "      <td>0.569466</td>\n",
       "      <td>0.581040</td>\n",
       "      <td>0.582569</td>\n",
       "      <td>0.591500</td>\n",
       "      <td>0.018162</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.559649</td>\n",
       "      <td>0.007770</td>\n",
       "      <td>0.029175</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 1, 'min_...</td>\n",
       "      <td>0.566412</td>\n",
       "      <td>0.603053</td>\n",
       "      <td>0.566412</td>\n",
       "      <td>0.562691</td>\n",
       "      <td>0.584098</td>\n",
       "      <td>0.576533</td>\n",
       "      <td>0.015212</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.992497</td>\n",
       "      <td>0.008299</td>\n",
       "      <td>0.038766</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 1, 'min_...</td>\n",
       "      <td>0.566412</td>\n",
       "      <td>0.604580</td>\n",
       "      <td>0.570992</td>\n",
       "      <td>0.564220</td>\n",
       "      <td>0.579511</td>\n",
       "      <td>0.577143</td>\n",
       "      <td>0.014686</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.409920</td>\n",
       "      <td>0.011050</td>\n",
       "      <td>0.046893</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 1, 'min_...</td>\n",
       "      <td>0.569466</td>\n",
       "      <td>0.610687</td>\n",
       "      <td>0.586260</td>\n",
       "      <td>0.581040</td>\n",
       "      <td>0.584098</td>\n",
       "      <td>0.586310</td>\n",
       "      <td>0.013496</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.569669</td>\n",
       "      <td>0.003811</td>\n",
       "      <td>0.030138</td>\n",
       "      <td>0.001883</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 2, 'min_...</td>\n",
       "      <td>0.572519</td>\n",
       "      <td>0.584733</td>\n",
       "      <td>0.549618</td>\n",
       "      <td>0.576453</td>\n",
       "      <td>0.576453</td>\n",
       "      <td>0.571955</td>\n",
       "      <td>0.011857</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.005191</td>\n",
       "      <td>0.008476</td>\n",
       "      <td>0.038211</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 2, 'min_...</td>\n",
       "      <td>0.567939</td>\n",
       "      <td>0.604580</td>\n",
       "      <td>0.541985</td>\n",
       "      <td>0.585627</td>\n",
       "      <td>0.597859</td>\n",
       "      <td>0.579598</td>\n",
       "      <td>0.022559</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.432640</td>\n",
       "      <td>0.010163</td>\n",
       "      <td>0.046957</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 2, 'min_...</td>\n",
       "      <td>0.581679</td>\n",
       "      <td>0.618321</td>\n",
       "      <td>0.551145</td>\n",
       "      <td>0.593272</td>\n",
       "      <td>0.607034</td>\n",
       "      <td>0.590290</td>\n",
       "      <td>0.023158</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.555660</td>\n",
       "      <td>0.002148</td>\n",
       "      <td>0.029231</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 2, 'min_...</td>\n",
       "      <td>0.589313</td>\n",
       "      <td>0.595420</td>\n",
       "      <td>0.561832</td>\n",
       "      <td>0.593272</td>\n",
       "      <td>0.577982</td>\n",
       "      <td>0.583564</td>\n",
       "      <td>0.012420</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.988572</td>\n",
       "      <td>0.012325</td>\n",
       "      <td>0.038074</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 2, 'min_...</td>\n",
       "      <td>0.604580</td>\n",
       "      <td>0.603053</td>\n",
       "      <td>0.572519</td>\n",
       "      <td>0.588685</td>\n",
       "      <td>0.585627</td>\n",
       "      <td>0.590893</td>\n",
       "      <td>0.011878</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.394247</td>\n",
       "      <td>0.008889</td>\n",
       "      <td>0.046757</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 2, 'min_...</td>\n",
       "      <td>0.583206</td>\n",
       "      <td>0.603053</td>\n",
       "      <td>0.586260</td>\n",
       "      <td>0.577982</td>\n",
       "      <td>0.579511</td>\n",
       "      <td>0.586002</td>\n",
       "      <td>0.009001</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.544714</td>\n",
       "      <td>0.005547</td>\n",
       "      <td>0.029173</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 2, 'min_...</td>\n",
       "      <td>0.560305</td>\n",
       "      <td>0.607634</td>\n",
       "      <td>0.564885</td>\n",
       "      <td>0.600917</td>\n",
       "      <td>0.573394</td>\n",
       "      <td>0.581427</td>\n",
       "      <td>0.019240</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.955758</td>\n",
       "      <td>0.012910</td>\n",
       "      <td>0.038018</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 2, 'min_...</td>\n",
       "      <td>0.580153</td>\n",
       "      <td>0.635115</td>\n",
       "      <td>0.574046</td>\n",
       "      <td>0.584098</td>\n",
       "      <td>0.565749</td>\n",
       "      <td>0.587832</td>\n",
       "      <td>0.024439</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.362186</td>\n",
       "      <td>0.010893</td>\n",
       "      <td>0.046774</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 2, 'min_...</td>\n",
       "      <td>0.570992</td>\n",
       "      <td>0.648855</td>\n",
       "      <td>0.583206</td>\n",
       "      <td>0.576453</td>\n",
       "      <td>0.574924</td>\n",
       "      <td>0.590886</td>\n",
       "      <td>0.029252</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.988680</td>\n",
       "      <td>0.008027</td>\n",
       "      <td>0.031017</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_leaf': 1, 'min_...</td>\n",
       "      <td>0.555725</td>\n",
       "      <td>0.567939</td>\n",
       "      <td>0.537405</td>\n",
       "      <td>0.559633</td>\n",
       "      <td>0.538226</td>\n",
       "      <td>0.551786</td>\n",
       "      <td>0.012072</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.848758</td>\n",
       "      <td>0.022446</td>\n",
       "      <td>0.042012</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_leaf': 1, 'min_...</td>\n",
       "      <td>0.572519</td>\n",
       "      <td>0.590840</td>\n",
       "      <td>0.561832</td>\n",
       "      <td>0.564220</td>\n",
       "      <td>0.553517</td>\n",
       "      <td>0.568586</td>\n",
       "      <td>0.012668</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.689232</td>\n",
       "      <td>0.024255</td>\n",
       "      <td>0.052611</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_leaf': 1, 'min_...</td>\n",
       "      <td>0.581679</td>\n",
       "      <td>0.606107</td>\n",
       "      <td>0.558779</td>\n",
       "      <td>0.558104</td>\n",
       "      <td>0.568807</td>\n",
       "      <td>0.574695</td>\n",
       "      <td>0.017882</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.954920</td>\n",
       "      <td>0.012750</td>\n",
       "      <td>0.030838</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_leaf': 1, 'min_...</td>\n",
       "      <td>0.590840</td>\n",
       "      <td>0.596947</td>\n",
       "      <td>0.593893</td>\n",
       "      <td>0.591743</td>\n",
       "      <td>0.545872</td>\n",
       "      <td>0.583859</td>\n",
       "      <td>0.019110</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.785414</td>\n",
       "      <td>0.019448</td>\n",
       "      <td>0.041767</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_leaf': 1, 'min_...</td>\n",
       "      <td>0.606107</td>\n",
       "      <td>0.598473</td>\n",
       "      <td>0.578626</td>\n",
       "      <td>0.585627</td>\n",
       "      <td>0.556575</td>\n",
       "      <td>0.585082</td>\n",
       "      <td>0.017181</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.643409</td>\n",
       "      <td>0.072572</td>\n",
       "      <td>0.052581</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_leaf': 1, 'min_...</td>\n",
       "      <td>0.606107</td>\n",
       "      <td>0.603053</td>\n",
       "      <td>0.584733</td>\n",
       "      <td>0.577982</td>\n",
       "      <td>0.556575</td>\n",
       "      <td>0.585690</td>\n",
       "      <td>0.018035</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.942618</td>\n",
       "      <td>0.020149</td>\n",
       "      <td>0.031033</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_leaf': 1, 'min_...</td>\n",
       "      <td>0.567939</td>\n",
       "      <td>0.625954</td>\n",
       "      <td>0.574046</td>\n",
       "      <td>0.558104</td>\n",
       "      <td>0.548930</td>\n",
       "      <td>0.574995</td>\n",
       "      <td>0.026878</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3.719356</td>\n",
       "      <td>0.013802</td>\n",
       "      <td>0.041240</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_leaf': 1, 'min_...</td>\n",
       "      <td>0.578626</td>\n",
       "      <td>0.603053</td>\n",
       "      <td>0.561832</td>\n",
       "      <td>0.568807</td>\n",
       "      <td>0.571865</td>\n",
       "      <td>0.576837</td>\n",
       "      <td>0.014176</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.502304</td>\n",
       "      <td>0.020412</td>\n",
       "      <td>0.051420</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_leaf': 1, 'min_...</td>\n",
       "      <td>0.589313</td>\n",
       "      <td>0.619847</td>\n",
       "      <td>0.552672</td>\n",
       "      <td>0.565749</td>\n",
       "      <td>0.582569</td>\n",
       "      <td>0.582030</td>\n",
       "      <td>0.022849</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.929972</td>\n",
       "      <td>0.014154</td>\n",
       "      <td>0.030698</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_leaf': 2, 'min_...</td>\n",
       "      <td>0.577099</td>\n",
       "      <td>0.589313</td>\n",
       "      <td>0.566412</td>\n",
       "      <td>0.565749</td>\n",
       "      <td>0.558104</td>\n",
       "      <td>0.571336</td>\n",
       "      <td>0.010834</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3.733999</td>\n",
       "      <td>0.022029</td>\n",
       "      <td>0.041328</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_leaf': 2, 'min_...</td>\n",
       "      <td>0.593893</td>\n",
       "      <td>0.581679</td>\n",
       "      <td>0.584733</td>\n",
       "      <td>0.591743</td>\n",
       "      <td>0.585627</td>\n",
       "      <td>0.587535</td>\n",
       "      <td>0.004559</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.532693</td>\n",
       "      <td>0.022177</td>\n",
       "      <td>0.051748</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_leaf': 2, 'min_...</td>\n",
       "      <td>0.590840</td>\n",
       "      <td>0.587786</td>\n",
       "      <td>0.566412</td>\n",
       "      <td>0.590214</td>\n",
       "      <td>0.579511</td>\n",
       "      <td>0.582953</td>\n",
       "      <td>0.009206</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        1.594552      0.015685         0.029260        0.000151   \n",
       "1        3.047000      0.013573         0.038398        0.000188   \n",
       "2        4.482934      0.008807         0.047124        0.000148   \n",
       "3        1.575366      0.004644         0.029168        0.000114   \n",
       "4        3.011917      0.011227         0.038349        0.000227   \n",
       "5        4.442428      0.014459         0.046950        0.000079   \n",
       "6        1.559649      0.007770         0.029175        0.000060   \n",
       "7        2.992497      0.008299         0.038766        0.001543   \n",
       "8        4.409920      0.011050         0.046893        0.000251   \n",
       "9        1.569669      0.003811         0.030138        0.001883   \n",
       "10       3.005191      0.008476         0.038211        0.000177   \n",
       "11       4.432640      0.010163         0.046957        0.000084   \n",
       "12       1.555660      0.002148         0.029231        0.000051   \n",
       "13       2.988572      0.012325         0.038074        0.000255   \n",
       "14       4.394247      0.008889         0.046757        0.000129   \n",
       "15       1.544714      0.005547         0.029173        0.000171   \n",
       "16       2.955758      0.012910         0.038018        0.000354   \n",
       "17       4.362186      0.010893         0.046774        0.000104   \n",
       "18       1.988680      0.008027         0.031017        0.000106   \n",
       "19       3.848758      0.022446         0.042012        0.000160   \n",
       "20       5.689232      0.024255         0.052611        0.000127   \n",
       "21       1.954920      0.012750         0.030838        0.000105   \n",
       "22       3.785414      0.019448         0.041767        0.000273   \n",
       "23       5.643409      0.072572         0.052581        0.000579   \n",
       "24       1.942618      0.020149         0.031033        0.000605   \n",
       "25       3.719356      0.013802         0.041240        0.000253   \n",
       "26       5.502304      0.020412         0.051420        0.000261   \n",
       "27       1.929972      0.014154         0.030698        0.000071   \n",
       "28       3.733999      0.022029         0.041328        0.000175   \n",
       "29       5.532693      0.022177         0.051748        0.000447   \n",
       "\n",
       "   param_max_depth param_min_samples_leaf param_min_samples_split  \\\n",
       "0               10                      1                       5   \n",
       "1               10                      1                       5   \n",
       "2               10                      1                       5   \n",
       "3               10                      1                       7   \n",
       "4               10                      1                       7   \n",
       "5               10                      1                       7   \n",
       "6               10                      1                       9   \n",
       "7               10                      1                       9   \n",
       "8               10                      1                       9   \n",
       "9               10                      2                       5   \n",
       "10              10                      2                       5   \n",
       "11              10                      2                       5   \n",
       "12              10                      2                       7   \n",
       "13              10                      2                       7   \n",
       "14              10                      2                       7   \n",
       "15              10                      2                       9   \n",
       "16              10                      2                       9   \n",
       "17              10                      2                       9   \n",
       "18              15                      1                       5   \n",
       "19              15                      1                       5   \n",
       "20              15                      1                       5   \n",
       "21              15                      1                       7   \n",
       "22              15                      1                       7   \n",
       "23              15                      1                       7   \n",
       "24              15                      1                       9   \n",
       "25              15                      1                       9   \n",
       "26              15                      1                       9   \n",
       "27              15                      2                       5   \n",
       "28              15                      2                       5   \n",
       "29              15                      2                       5   \n",
       "\n",
       "   param_n_estimators                                             params  \\\n",
       "0                  50  {'max_depth': 10, 'min_samples_leaf': 1, 'min_...   \n",
       "1                 100  {'max_depth': 10, 'min_samples_leaf': 1, 'min_...   \n",
       "2                 150  {'max_depth': 10, 'min_samples_leaf': 1, 'min_...   \n",
       "3                  50  {'max_depth': 10, 'min_samples_leaf': 1, 'min_...   \n",
       "4                 100  {'max_depth': 10, 'min_samples_leaf': 1, 'min_...   \n",
       "5                 150  {'max_depth': 10, 'min_samples_leaf': 1, 'min_...   \n",
       "6                  50  {'max_depth': 10, 'min_samples_leaf': 1, 'min_...   \n",
       "7                 100  {'max_depth': 10, 'min_samples_leaf': 1, 'min_...   \n",
       "8                 150  {'max_depth': 10, 'min_samples_leaf': 1, 'min_...   \n",
       "9                  50  {'max_depth': 10, 'min_samples_leaf': 2, 'min_...   \n",
       "10                100  {'max_depth': 10, 'min_samples_leaf': 2, 'min_...   \n",
       "11                150  {'max_depth': 10, 'min_samples_leaf': 2, 'min_...   \n",
       "12                 50  {'max_depth': 10, 'min_samples_leaf': 2, 'min_...   \n",
       "13                100  {'max_depth': 10, 'min_samples_leaf': 2, 'min_...   \n",
       "14                150  {'max_depth': 10, 'min_samples_leaf': 2, 'min_...   \n",
       "15                 50  {'max_depth': 10, 'min_samples_leaf': 2, 'min_...   \n",
       "16                100  {'max_depth': 10, 'min_samples_leaf': 2, 'min_...   \n",
       "17                150  {'max_depth': 10, 'min_samples_leaf': 2, 'min_...   \n",
       "18                 50  {'max_depth': 15, 'min_samples_leaf': 1, 'min_...   \n",
       "19                100  {'max_depth': 15, 'min_samples_leaf': 1, 'min_...   \n",
       "20                150  {'max_depth': 15, 'min_samples_leaf': 1, 'min_...   \n",
       "21                 50  {'max_depth': 15, 'min_samples_leaf': 1, 'min_...   \n",
       "22                100  {'max_depth': 15, 'min_samples_leaf': 1, 'min_...   \n",
       "23                150  {'max_depth': 15, 'min_samples_leaf': 1, 'min_...   \n",
       "24                 50  {'max_depth': 15, 'min_samples_leaf': 1, 'min_...   \n",
       "25                100  {'max_depth': 15, 'min_samples_leaf': 1, 'min_...   \n",
       "26                150  {'max_depth': 15, 'min_samples_leaf': 1, 'min_...   \n",
       "27                 50  {'max_depth': 15, 'min_samples_leaf': 2, 'min_...   \n",
       "28                100  {'max_depth': 15, 'min_samples_leaf': 2, 'min_...   \n",
       "29                150  {'max_depth': 15, 'min_samples_leaf': 2, 'min_...   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0            0.596947           0.586260           0.554198   \n",
       "1            0.598473           0.592366           0.560305   \n",
       "2            0.590840           0.616794           0.558779   \n",
       "3            0.595420           0.607634           0.563359   \n",
       "4            0.596947           0.609160           0.560305   \n",
       "5            0.604580           0.619847           0.569466   \n",
       "6            0.566412           0.603053           0.566412   \n",
       "7            0.566412           0.604580           0.570992   \n",
       "8            0.569466           0.610687           0.586260   \n",
       "9            0.572519           0.584733           0.549618   \n",
       "10           0.567939           0.604580           0.541985   \n",
       "11           0.581679           0.618321           0.551145   \n",
       "12           0.589313           0.595420           0.561832   \n",
       "13           0.604580           0.603053           0.572519   \n",
       "14           0.583206           0.603053           0.586260   \n",
       "15           0.560305           0.607634           0.564885   \n",
       "16           0.580153           0.635115           0.574046   \n",
       "17           0.570992           0.648855           0.583206   \n",
       "18           0.555725           0.567939           0.537405   \n",
       "19           0.572519           0.590840           0.561832   \n",
       "20           0.581679           0.606107           0.558779   \n",
       "21           0.590840           0.596947           0.593893   \n",
       "22           0.606107           0.598473           0.578626   \n",
       "23           0.606107           0.603053           0.584733   \n",
       "24           0.567939           0.625954           0.574046   \n",
       "25           0.578626           0.603053           0.561832   \n",
       "26           0.589313           0.619847           0.552672   \n",
       "27           0.577099           0.589313           0.566412   \n",
       "28           0.593893           0.581679           0.584733   \n",
       "29           0.590840           0.587786           0.566412   \n",
       "\n",
       "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "0            0.579511           0.567278         0.576839        0.014861   \n",
       "1            0.590214           0.585627         0.585397        0.013209   \n",
       "2            0.585627           0.602446         0.590897        0.019305   \n",
       "3            0.588685           0.570336         0.585087        0.016234   \n",
       "4            0.582569           0.585627         0.586922        0.016273   \n",
       "5            0.581040           0.582569         0.591500        0.018162   \n",
       "6            0.562691           0.584098         0.576533        0.015212   \n",
       "7            0.564220           0.579511         0.577143        0.014686   \n",
       "8            0.581040           0.584098         0.586310        0.013496   \n",
       "9            0.576453           0.576453         0.571955        0.011857   \n",
       "10           0.585627           0.597859         0.579598        0.022559   \n",
       "11           0.593272           0.607034         0.590290        0.023158   \n",
       "12           0.593272           0.577982         0.583564        0.012420   \n",
       "13           0.588685           0.585627         0.590893        0.011878   \n",
       "14           0.577982           0.579511         0.586002        0.009001   \n",
       "15           0.600917           0.573394         0.581427        0.019240   \n",
       "16           0.584098           0.565749         0.587832        0.024439   \n",
       "17           0.576453           0.574924         0.590886        0.029252   \n",
       "18           0.559633           0.538226         0.551786        0.012072   \n",
       "19           0.564220           0.553517         0.568586        0.012668   \n",
       "20           0.558104           0.568807         0.574695        0.017882   \n",
       "21           0.591743           0.545872         0.583859        0.019110   \n",
       "22           0.585627           0.556575         0.585082        0.017181   \n",
       "23           0.577982           0.556575         0.585690        0.018035   \n",
       "24           0.558104           0.548930         0.574995        0.026878   \n",
       "25           0.568807           0.571865         0.576837        0.014176   \n",
       "26           0.565749           0.582569         0.582030        0.022849   \n",
       "27           0.565749           0.558104         0.571336        0.010834   \n",
       "28           0.591743           0.585627         0.587535        0.004559   \n",
       "29           0.590214           0.579511         0.582953        0.009206   \n",
       "\n",
       "    rank_test_score  \n",
       "0                25  \n",
       "1                13  \n",
       "2                 2  \n",
       "3                14  \n",
       "4                 9  \n",
       "5                 1  \n",
       "6                27  \n",
       "7                24  \n",
       "8                10  \n",
       "9                33  \n",
       "10               22  \n",
       "11                5  \n",
       "12               17  \n",
       "13                3  \n",
       "14               11  \n",
       "15               20  \n",
       "16                7  \n",
       "17                4  \n",
       "18               36  \n",
       "19               35  \n",
       "20               30  \n",
       "21               16  \n",
       "22               15  \n",
       "23               12  \n",
       "24               29  \n",
       "25               26  \n",
       "26               19  \n",
       "27               34  \n",
       "28                8  \n",
       "29               18  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "# https://towardsdatascience.com/optimizing-hyperparameters-in-random-forest-classification-ec7741f9d3f6\n",
    "\n",
    "hyperparams = {'n_estimators' : [50, 100, 150],\n",
    "               'max_depth' : [10, 15],\n",
    "               'min_samples_split' : [5, 7, 9],\n",
    "               'min_samples_leaf' : [1, 2] \n",
    "              }\n",
    "\n",
    "gs_model = GridSearchCV(RandomForestClassifier(random_state=SEED), hyperparams, cv = cv)\n",
    "\n",
    "gs_model.fit(X, y)\n",
    "\n",
    "print('Melhores parametros : ', gs_model.best_params_)\n",
    "\n",
    "print('Melhor score: ', gs_model.best_score_)\n",
    "\n",
    "results = pd.DataFrame(gs_model.cv_results_)\n",
    "\n",
    "results.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parametros :  {'n_estimators': 200, 'min_samples_split': 12, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 8, 'bootstrap': False}\n",
      "Melhor score:  0.5915003384924248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, max_depth=8, max_features='sqrt',\n",
       "                       min_samples_leaf=4, min_samples_split=12,\n",
       "                       n_estimators=200, random_state=46)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams = {'n_estimators' : [50, 100, 150, 200], # number of trees in the foreset\n",
    "               'max_features' : ['auto', 'sqrt'], # Method considered for splitting a node\n",
    "               'max_depth' : [8, 10, 15, 20], # Maximum number of levels in each decision tree\n",
    "               'min_samples_split' : [2, 5, 8, 12], # Minimum number of data points placed in a node before the node is split\n",
    "               'min_samples_leaf' : [1, 2, 3, 4, 5], # Minimum number of data points allowed in a leaf node\n",
    "               'bootstrap' : [True, False] # Method for sampling data points (with or without replacement)\n",
    "              }\n",
    "\n",
    "rs_model = RandomizedSearchCV(RandomForestClassifier(random_state=SEED), hyperparams, cv = cv, n_iter = 100)\n",
    "\n",
    "rs_model.fit(X, y)\n",
    "\n",
    "print('Melhores parametros : ', rs_model.best_params_)\n",
    "print('Melhor score: ', gs_model.best_score_)\n",
    "rs_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(model, test_features, test_labels):\n",
    "#     predictions = model.predict(test_features)\n",
    "#     errors = abs(predictions - test_labels)\n",
    "#     mape = 100 * np.mean(errors / test_labels)\n",
    "#     accuracy = 100 - mape\n",
    "#     print('Model Performance')\n",
    "#     print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "#     print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "#     return accuracy\n",
    "\n",
    "# base_model = rfr_model\n",
    "# base_model.fit(train_features, train_labels)\n",
    "# base_accuracy = evaluate(base_model, test_features, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
