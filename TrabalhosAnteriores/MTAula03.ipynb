{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aula 03 - Part-Of-Speech Tagging ou Anota√ß√£o Morfossint√°tica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Avell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Avell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Avell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "------- SENTENCAS -------\n",
      "['@USER @USER go home you‚Äôre drunk @USER #MAGA #Trump2020 üëäüá∫üá∏üëä URL !!', '!']\n",
      " \n",
      "------- TOKENIZACAO DE SENTENCAS -------\n",
      "\n",
      "------- WordTokenizer -------\n",
      "\n",
      "['@', 'USER', '@', 'USER', 'go', 'home', 'you', '‚Äô', 're', 'drunk', '@', 'USER', '#', 'MAGA', '#', 'Trump2020', 'üëäüá∫üá∏üëä', 'URL', '!', '!']\n",
      "\n",
      "\n",
      "------- RegExpTokenizer -------\n",
      "\n",
      "['@USER @USER go home you‚Äôre drunk @USER #MAGA #Trump2020 üëäüá∫üá∏üëä URL ']\n",
      "\n",
      "\n",
      "------- WordPunctTokenizer -------\n",
      "\n",
      "['@', 'USER', '@', 'USER', 'go', 'home', 'you', '‚Äô', 're', 'drunk', '@', 'USER', '#', 'MAGA', '#', 'Trump2020', 'üëäüá∫üá∏üëä', 'URL', '!!']\n",
      "\n",
      "\n",
      "------- TweetTokenizer -------\n",
      "\n",
      "['@USER', '@USER', 'go', 'home', 'you', '‚Äô', 're', 'drunk', '@USER', '#MAGA', '#Trump2020', 'üëä', 'üá∫', 'üá∏', 'üëä', 'URL', '!', '!']\n",
      "\n",
      "\n",
      " \n",
      "------- LISTA DE STOP WORDS -------\n",
      "\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Avell/nltk_data'\n    - 'C:\\\\Users\\\\Avell\\\\.conda\\\\envs\\\\Unifor\\\\nltk_data'\n    - 'C:\\\\Users\\\\Avell\\\\.conda\\\\envs\\\\Unifor\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Avell\\\\.conda\\\\envs\\\\Unifor\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Avell\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\Unifor\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Unifor\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Avell/nltk_data'\n    - 'C:\\\\Users\\\\Avell\\\\.conda\\\\envs\\\\Unifor\\\\nltk_data'\n    - 'C:\\\\Users\\\\Avell\\\\.conda\\\\envs\\\\Unifor\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Avell\\\\.conda\\\\envs\\\\Unifor\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Avell\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-65710154fcd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'------- LISTA DE STOP WORDS -------\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Unifor\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Unifor\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Unifor\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Unifor\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Avell/nltk_data'\n    - 'C:\\\\Users\\\\Avell\\\\.conda\\\\envs\\\\Unifor\\\\nltk_data'\n    - 'C:\\\\Users\\\\Avell\\\\.conda\\\\envs\\\\Unifor\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Avell\\\\.conda\\\\envs\\\\Unifor\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Avell\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "texto1 = \"@USER Canada doesn‚Äôt need another CUCK! We already have enough #LooneyLeft #Liberals f**king up our great country!!! #Qproofs #TrudeauMustGo\"\n",
    "texto = \"@USER @USER go home you‚Äôre drunk @USER #MAGA #Trump2020 üëäüá∫üá∏üëä URL !!!\"\n",
    "\n",
    "# Importando os m√≥dulos necess√°rios\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import TweetTokenizer \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Tokenizando as senten√ßas do texto\n",
    "sentences = sent_tokenize(texto)\n",
    "print(' ')\n",
    "print('------- SENTENCAS -------')\n",
    "print(sentences)\n",
    "\n",
    "# Removendo stop-words\n",
    "# Muitas vezes na fase de pr√©-processamento de um dataset √© preciso remover dados n√£o-uteis. \n",
    "# Em NLP, estes dados n√£o-uteis s√£o comumente referenciados por stop-words. Geralmente s√£o palavras das classes fechadas\n",
    "# como artigos, preposicoes, conjun√ß√µes. Mas, obviamente, podem variar confome o dom√≠nio da aplica√ß√£o.\n",
    "\n",
    "print(' ')\n",
    "print('------- TOKENIZACAO DE SENTENCAS -------\\n')\n",
    "\n",
    "print('------- WordTokenizer -------\\n')\n",
    "tokenized_sent = word_tokenize(sentences[0])\n",
    "print(tokenized_sent)\n",
    "print(\"\\n\")\n",
    "\n",
    "print('------- RegExpTokenizer -------\\n')\n",
    "pontuacao = r\"[.?:!]+\"\n",
    "tk = RegexpTokenizer(pontuacao, gaps = True) \n",
    "tokenized_sent1 = tk.tokenize(sentences[0]) \n",
    "print(tokenized_sent1)\n",
    "print(\"\\n\")\n",
    "\n",
    "print('------- WordPunctTokenizer -------\\n')\n",
    "punct_tokenizer = WordPunctTokenizer()\n",
    "tokenized_sent2 = punct_tokenizer.tokenize(sentences[0])\n",
    "print(tokenized_sent2)\n",
    "print(\"\\n\")\n",
    "\n",
    "print('------- TweetTokenizer -------\\n')\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tokenized_sent3 = tweet_tokenizer.tokenize(sentences[0])\n",
    "print(tokenized_sent3)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(' ')\n",
    "print('------- LISTA DE STOP WORDS -------\\n')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "print(stop_words)\n",
    "\n",
    "filtered_tokens = [w for w in tokenized_sent3 if not w in stop_words] \n",
    " \n",
    "print(' ')\n",
    "print('------- TOKENS DA 1a. SENTENCA (com stop-words) -------\\n')\n",
    "print(tokenized_sent3)\n",
    "print(' ')\n",
    "print('------- TOKENS DA 1a. SENTENCA (sem stop-words) -------\\n')\n",
    "print(filtered_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "------- POS TAGGING dos TOKENS DA 1a. SENTENCA (sem stop-words) -------\n",
      "\n",
      "[('@USER', 'VB'), ('@USER', 'NNP'), ('go', 'VB'), ('home', 'NN'), ('‚Äô', 'NNP'), ('drunk', 'NN'), ('@USER', 'NNP'), ('#MAGA', 'NNP'), ('#Trump2020', 'NNP'), ('üëä', 'NNP'), ('üá∫', 'NNP'), ('üá∏', 'NNP'), ('üëä', 'NNP'), ('URL', 'NNP'), ('!', '.'), ('!', '.')]\n",
      " \n",
      "------- Stem ou Raiz de TOKENS da senten√ßa exemplo -------\n",
      "\n",
      "programers  :  program\n",
      "program  :  program\n",
      "with  :  with\n",
      "better  :  better\n",
      "programing  :  program\n",
      "languages  :  languag\n",
      " \n",
      "------- Stem ou Raiz de TOKENS DA 1a. SENTENCA (sem stop-words) -------\n",
      "\n",
      "@USER  :  @user\n",
      "@USER  :  @user\n",
      "go  :  go\n",
      "home  :  home\n",
      "‚Äô  :  ‚Äô\n",
      "drunk  :  drunk\n",
      "@USER  :  @user\n",
      "#MAGA  :  #maga\n",
      "#Trump2020  :  #trump2020\n",
      "üëä  :  üëä\n",
      "üá∫  :  üá∫\n",
      "üá∏  :  üá∏\n",
      "üëä  :  üëä\n",
      "URL  :  url\n",
      "!  :  !\n",
      "!  :  !\n",
      " \n",
      "------- Testes do Lemmatizer -------\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lemmatizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d27238c823f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'------- Testes do Lemmatizer -------\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# a denotes adjective in \"pos\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rocks :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rocks\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"corpora :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"corpora\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"better :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"better\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lemmatizer' is not defined"
     ]
    }
   ],
   "source": [
    "# POS TAGGING - Anota√ß√£o Morfossint√°tica\n",
    "\n",
    "# Basicamente, o objetivo √© associar informa√ß√£o lingu√≠stica (classe gramatical e modificadores) a cada unidade \n",
    "# subsentencial ou tokens, que, por sua vez, s√£o palavras, express√µes multi-palavras e simbolos ($, !, ?, ;, etc).\n",
    "# Cada POS Tagger possui seu conjunto de tags proprio.\n",
    "\n",
    "# CC coordinating conjunction\n",
    "# CD cardinal digit\n",
    "# DT determiner\n",
    "# EX existential there (like: ‚Äúthere is‚Äù ‚Ä¶ think of it like ‚Äúthere exists‚Äù)\n",
    "# FW foreign word\n",
    "# IN preposition/subordinating conjunction\n",
    "# JJ adjective ‚Äòbig‚Äô\n",
    "# JJR adjective, comparative ‚Äòbigger‚Äô\n",
    "# JJS adjective, superlative ‚Äòbiggest‚Äô\n",
    "# LS list marker 1)\n",
    "# MD modal could, will\n",
    "# NN noun, singular ‚Äòdesk‚Äô\n",
    "# NNS noun plural ‚Äòdesks‚Äô\n",
    "# NNP proper noun, singular ‚ÄòHarrison‚Äô\n",
    "# NNPS proper noun, plural ‚ÄòAmericans‚Äô\n",
    "# PDT predeterminer ‚Äòall the kids‚Äô\n",
    "# POS possessive ending parent‚Äòs\n",
    "# PRP personal pronoun I, he, she\n",
    "# PRP$ possessive pronoun my, his, hers\n",
    "# RB adverb very, silently,\n",
    "# RBR adverb, comparative better\n",
    "# RBS adverb, superlative best\n",
    "# RP particle give up\n",
    "# TO to go ‚Äòto‚Äò the store.\n",
    "# UH interjection errrrrrrrm\n",
    "# VB verb, base form take\n",
    "# VBD verb, past tense took\n",
    "# VBG verb, gerund/present participle taking\n",
    "# VBN verb, past participle taken\n",
    "# VBP verb, sing. present, non-3d take\n",
    "# VBZ verb, 3rd person sing. present takes\n",
    "# WDT wh-determiner which\n",
    "# WP wh-pronoun who, what\n",
    "# WP$ possessive wh-pronoun whose\n",
    "# WRB wh-abverb where, when\n",
    "\n",
    "print(' ')\n",
    "print('------- POS TAGGING dos TOKENS DA 1a. SENTENCA (sem stop-words) -------\\n')\n",
    "sent_tagged = nltk.pos_tag(filtered_tokens) \n",
    "print(sent_tagged) \n",
    "\n",
    "# Stemming - processo de extra√ß√£o do radical ou raiz de um TOKEN\n",
    "ps = PorterStemmer() \n",
    "   \n",
    "sentence = \"programers program with better programing languages\"\n",
    "words = word_tokenize(sentence) \n",
    "\n",
    "print(' ')\n",
    "print('------- Stem ou Raiz de TOKENS da senten√ßa exemplo -------\\n')\n",
    "for w in words: \n",
    "    print(w, \" : \", ps.stem(w)) \n",
    "    \n",
    "print(' ')\n",
    "print('------- Stem ou Raiz de TOKENS DA 1a. SENTENCA (sem stop-words) -------\\n')\n",
    "for w in filtered_tokens: \n",
    "    print(w, \" : \", ps.stem(w)) \n",
    "    \n",
    "\n",
    "# Lemmatizer: processo de agrupar formas flexionadas em uma s√≥ palavra com similar. \n",
    "# As diversas palavras (varia√ß√µes de genero, numero, tempo e modo verbal, etc) tem significado similar ao lema\n",
    " \n",
    "print(' ')\n",
    "print('------- Testes do Lemmatizer -------\\n')    \n",
    "# a denotes adjective in \"pos\" \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\")) \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\")) \n",
    "print(\"got :\", lemmatizer.lemmatize(\"got\", pos =\"v\"))\n",
    "print(\"drunk :\", lemmatizer.lemmatize(\"drunk\", pos =\"n\"))\n",
    "print(\"drunk :\", lemmatizer.lemmatize(\"drunk\", pos =\"v\"))\n",
    "print(\"programers :\", lemmatizer.lemmatize(\"programers\"))\n",
    "\n",
    "print(' ')\n",
    "print('------- Lema dos TOKENS DA 1a. SENTENCA (sem stop-words) -------\\n')    \n",
    "for w in filtered_tokens: \n",
    "    print(w, \" : \", lemmatizer.lemmatize(w)) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
